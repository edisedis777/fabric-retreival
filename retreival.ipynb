{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft Fabric Items Retrieval and Management System\n",
    "# Last Updated: March 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Purpose: \n",
    "#   - Retrieve all Fabric items efficiently with minimal Capacity Unit usage\n",
    "#   - Track changes over time in Fabric item inventory\n",
    "#   - Provide data quality validation for retrieved items\n",
    "#   - Enable incremental refresh capabilities\n",
    "#   - Implement alerting for significant changes\n",
    "# \n",
    "# IMPORTANT NOTES:\n",
    "#   - As of March 2025, Dataflow Gen2 is not included in the Fabric items\n",
    "#   - This only retrieves Fabric Items, not Power BI Items\n",
    "#   - Using this Python notebook uses significantly less Capacity Units than a Spark notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebookutils\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime, date, timedelta\n",
    "import time\n",
    "import duckdb\n",
    "from deltalake import write_deltalake, DeltaTable, read_deltalake\n",
    "import pyarrow\n",
    "import os\n",
    "import logging\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import hashlib\n",
    "from typing import Dict, List, Optional, Any, Tuple, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 1: Configuration and Logging Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration dictionary to centralize all settings\n",
    "config = {\n",
    "    # API and authentication settings\n",
    "    \"key_vault_url\": \"https://company-keyvault.vault.azure.net/\",\n",
    "    \"tenant_id_secret_name\": \"tenantid\",\n",
    "    \"client_id_secret_name\": \"powerbi-applicationid\",\n",
    "    \"client_secret_name\": \"powerbi-clientsecret\",\n",
    "    \"fabric_api_base_url\": \"https://api.fabric.microsoft.com/v1/admin/items\",\n",
    "    \"api_scope\": \"https://analysis.windows.net/powerbi/api/.default\",\n",
    "    \n",
    "    # Storage paths\n",
    "    \"raw_json_dir\": \"/lakehouse/default/Files/Fabric_Items/\",\n",
    "    \"staging_table_path\": \"/lakehouse/default/Tables/staging_all_fabric_items\",\n",
    "    \"production_table_path\": \"/lakehouse/default/Tables/all_fabric_items\",\n",
    "    \"changes_table_path\": \"/lakehouse/default/Tables/fabric_items_changes\",\n",
    "    \"history_table_path\": \"/lakehouse/default/Tables/fabric_items_history\",\n",
    "    \"quality_issues_path\": \"/lakehouse/default/Tables/fabric_items_quality_issues\",\n",
    "    \n",
    "    # Runtime configurations\n",
    "    \"incremental_refresh\": True,  # Set to False for full refresh\n",
    "    \"days_to_keep_history\": 90,   # Number of days to retain historical data\n",
    "    \"request_delay\": 0.5,         # Delay between API requests in seconds\n",
    "    \"batch_size\": 1000,           # Number of items to process in memory at once\n",
    "    \n",
    "    # Alert settings\n",
    "    \"enable_alerts\": True,\n",
    "    \"alert_email_recipients\": [\"data.team@company.com\", \"fabric.admins@company.com\"],\n",
    "    \"alert_smtp_server\": \"smtp.company.com\",\n",
    "    \"alert_smtp_port\": 587,\n",
    "    \"alert_sender_email\": \"fabric-monitor@company.com\",\n",
    "    \"alert_threshold_item_count_change_pct\": 10,  # Alert if item count changes by 10%\n",
    "    \n",
    "    # Data quality thresholds\n",
    "    \"min_expected_items\": 100,    # Minimum expected number of items\n",
    "    \"required_fields\": [\"id\", \"name\", \"type\", \"modifiedBy\", \"modifiedDate\"],\n",
    "    \"valid_item_types\": [\"Report\", \"Dataset\", \"Dataflow\", \"Lakehouse\", \"Warehouse\", \"Dashboard\"]\n",
    "}\n",
    "\n",
    "# Set up logging configuration\n",
    "def setup_logging():\n",
    "    \"\"\"Configure the logging system for tracking execution and errors\"\"\"\n",
    "    \n",
    "    # Create directory for logs if it doesn't exist\n",
    "    notebookutils.fs.mkdirs(\"Files/Logs/\")\n",
    "    \n",
    "    # Set up logging format and level\n",
    "    log_file = f\"/lakehouse/default/Files/Logs/fabric_items_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()  # Also log to console/notebook output\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Logging initialized\")\n",
    "    return log_file\n",
    "\n",
    "# Initialize logging\n",
    "log_file_path = setup_logging()\n",
    "logging.info(f\"Starting Fabric Items retrieval and processing job\")\n",
    "logging.info(f\"Using configuration: {json.dumps({k: v for k, v in config.items() if not 'secret' in k})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2: Authentication and Key Vault Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auth_token() -> str:\n",
    "    \"\"\"\n",
    "    Retrieve authentication credentials from Key Vault and obtain access token\n",
    "    \n",
    "    Returns:\n",
    "        str: The access token for API authentication\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If authentication fails for any reason\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Retrieving secrets from Azure Key Vault\")\n",
    "        \n",
    "        # Retrieve secrets from Azure Key Vault\n",
    "        key_vault = config[\"key_vault_url\"]\n",
    "        tenant = notebookutils.credentials.getSecret(key_vault, config[\"tenant_id_secret_name\"])\n",
    "        client = notebookutils.credentials.getSecret(key_vault, config[\"client_id_secret_name\"])\n",
    "        client_secret = notebookutils.credentials.getSecret(key_vault, config[\"client_secret_name\"])\n",
    "        \n",
    "        logging.info(\"Successfully retrieved secrets from Key Vault\")\n",
    "        \n",
    "        # Import required authentication libraries with error handling\n",
    "        try: \n",
    "            from azure.identity import ClientSecretCredential \n",
    "        except ImportError:\n",
    "            logging.info(\"Installing azure.identity package\")\n",
    "            %pip install azure.identity \n",
    "            from azure.identity import ClientSecretCredential\n",
    "            logging.info(\"Successfully installed azure.identity\")\n",
    "        \n",
    "        # Create the credential object for service principal authentication\n",
    "        auth = ClientSecretCredential(\n",
    "            authority='https://login.microsoftonline.com/', \n",
    "            tenant_id=tenant, \n",
    "            client_id=client, \n",
    "            client_secret=client_secret\n",
    "        )\n",
    "        \n",
    "        # Retrieve the access token\n",
    "        api = config[\"api_scope\"]\n",
    "        access_token = auth.get_token(api)\n",
    "        access_token = access_token.token\n",
    "        \n",
    "        logging.info(\"Successfully obtained authentication token\")\n",
    "        return access_token\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Authentication failed: {str(e)}\")\n",
    "        raise Exception(f\"Failed to authenticate: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 3: Data Retrieval from Fabric API with Incremental Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_modified_date() -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Get the most recent modifiedDate from the existing production table\n",
    "    for use with incremental refresh.\n",
    "    \n",
    "    Returns:\n",
    "        Optional[str]: ISO formatted date string of last modified date or None if no data exists\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the production table exists\n",
    "        if not notebookutils.fs.exists(config[\"production_table_path\"]):\n",
    "            logging.info(\"No existing production data found, performing full refresh\")\n",
    "            return None\n",
    "            \n",
    "        # Read the production table into a DataFrame\n",
    "        df = read_deltalake(config[\"production_table_path\"])\n",
    "        \n",
    "        # Convert to pandas for easier manipulation\n",
    "        pdf = df.to_pandas()\n",
    "        \n",
    "        if \"modifiedDate\" not in pdf.columns or pdf.empty:\n",
    "            logging.info(\"No modifiedDate column found or empty table, performing full refresh\")\n",
    "            return None\n",
    "            \n",
    "        # Get the maximum modifiedDate\n",
    "        last_modified = pdf[\"modifiedDate\"].max()\n",
    "        \n",
    "        # Convert to ISO format string if needed\n",
    "        if isinstance(last_modified, pd.Timestamp):\n",
    "            last_modified = last_modified.isoformat()\n",
    "            \n",
    "        logging.info(f\"Found last modified date: {last_modified}\")\n",
    "        return last_modified\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error determining last modified date: {str(e)}\")\n",
    "        logging.warning(\"Defaulting to full refresh\")\n",
    "        return None\n",
    "\n",
    "def retrieve_fabric_items(access_token: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve Fabric items from the API with support for full or incremental retrieval\n",
    "    \n",
    "    Args:\n",
    "        access_token (str): Authentication token for the API\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of Fabric items retrieved from the API\n",
    "    \"\"\"\n",
    "    # Set up headers with authentication token and content type\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # Initialize an empty list to store all Fabric items\n",
    "    all_fabric_items = []\n",
    "    \n",
    "    # Determine if we're doing incremental or full refresh\n",
    "    if config[\"incremental_refresh\"]:\n",
    "        last_modified_date = get_last_modified_date()\n",
    "        if last_modified_date:\n",
    "            # Modify the API URL to include a filter for modified date\n",
    "            # Note: This assumes the Fabric API supports filtering by modifiedDate\n",
    "            # If the API doesn't support this, we'll need to filter post-retrieval\n",
    "            base_url = config[\"fabric_api_base_url\"]\n",
    "            current_api_url = f\"{base_url}?$filter=modifiedDate gt {last_modified_date}\"\n",
    "            logging.info(f\"Using incremental refresh mode with modified date filter: {last_modified_date}\")\n",
    "        else:\n",
    "            current_api_url = config[\"fabric_api_base_url\"]\n",
    "            logging.info(\"Incremental refresh enabled but no existing data found. Performing full refresh.\")\n",
    "    else:\n",
    "        current_api_url = config[\"fabric_api_base_url\"]\n",
    "        logging.info(\"Using full refresh mode\")\n",
    "    \n",
    "    # Implement pagination to retrieve all items\n",
    "    page_count = 0\n",
    "    total_item_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Begin pagination loop\n",
    "    logging.info(f\"Starting API data retrieval from: {current_api_url}\")\n",
    "    \n",
    "    while current_api_url:\n",
    "        try:\n",
    "            # Make the API request for the current page\n",
    "            response = requests.get(current_api_url, headers=headers)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code != 200:\n",
    "                logging.error(f\"API request failed: Status code {response.status_code}\")\n",
    "                logging.error(f\"Response: {response.text}\")\n",
    "                \n",
    "                # If we have a 429 (Too Many Requests), wait longer and retry\n",
    "                if response.status_code == 429:\n",
    "                    retry_after = int(response.headers.get('Retry-After', '30'))\n",
    "                    logging.info(f\"Rate limited. Waiting {retry_after} seconds before retrying.\")\n",
    "                    time.sleep(retry_after)\n",
    "                    continue\n",
    "                    \n",
    "                # For other errors, break the loop after logging\n",
    "                break\n",
    "            \n",
    "            # Parse the JSON response\n",
    "            data = response.json()\n",
    "            page_count += 1\n",
    "            \n",
    "            # Extract and store the Fabric items from the current page\n",
    "            items_in_page = data.get('itemEntities', [])\n",
    "            page_item_count = len(items_in_page)\n",
    "            all_fabric_items.extend(items_in_page)\n",
    "            \n",
    "            total_item_count += page_item_count\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            logging.info(f\"Retrieved page {page_count} with {page_item_count} items. \" \n",
    "                        f\"Total: {total_item_count} items in {elapsed_time:.2f} seconds\")\n",
    "            \n",
    "            # Get the URL for the next page of results, if available\n",
    "            current_api_url = data.get('@odata.nextLink')\n",
    "            \n",
    "            # Add a small delay to avoid rate limiting\n",
    "            if current_api_url and config[\"request_delay\"] > 0:\n",
    "                time.sleep(config[\"request_delay\"])\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during API retrieval: {str(e)}\")\n",
    "            # If there's an error, wait and retry this page\n",
    "            logging.info(\"Waiting 30 seconds before retrying...\")\n",
    "            time.sleep(30)\n",
    "            # If we've had multiple failures, break the loop\n",
    "            # This would need additional implementation for proper retry logic\n",
    "    \n",
    "    # Log summary of the retrieval process\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(f\"API retrieval complete: {total_item_count} items retrieved across {page_count} pages \"\n",
    "                f\"in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return all_fabric_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 4: Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_quality(items: List[Dict]) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Validate the quality of retrieved data and separate valid items from issues\n",
    "    \n",
    "    Args:\n",
    "        items (List[Dict]): The list of Fabric items to validate\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: Valid items and items with quality issues\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting data quality validation for {len(items)} items\")\n",
    "    \n",
    "    valid_items = []\n",
    "    quality_issues = []\n",
    "    \n",
    "    # Define required fields based on configuration\n",
    "    required_fields = set(config[\"required_fields\"])\n",
    "    valid_types = set(config[\"valid_item_types\"])\n",
    "    \n",
    "    # Check if we have the minimum expected number of items\n",
    "    if len(items) < config[\"min_expected_items\"]:\n",
    "        issue = {\n",
    "            \"issue_type\": \"insufficient_items\",\n",
    "            \"description\": f\"Retrieved only {len(items)} items, which is below the minimum threshold of {config['min_expected_items']}\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"severity\": \"high\"\n",
    "        }\n",
    "        quality_issues.append(issue)\n",
    "        logging.warning(f\"Data quality issue: {issue['description']}\")\n",
    "    \n",
    "    # Process each item for validation\n",
    "    for item in items:\n",
    "        item_issues = []\n",
    "        \n",
    "        # Check for missing required fields\n",
    "        missing_fields = required_fields - set(item.keys())\n",
    "        if missing_fields:\n",
    "            item_issues.append({\n",
    "                \"issue_type\": \"missing_fields\",\n",
    "                \"fields\": list(missing_fields),\n",
    "                \"item_id\": item.get(\"id\", \"unknown\")\n",
    "            })\n",
    "        \n",
    "        # Check for valid item type if the field exists\n",
    "        if \"type\" in item and item[\"type\"] not in valid_types:\n",
    "            item_issues.append({\n",
    "                \"issue_type\": \"invalid_type\",\n",
    "                \"actual_type\": item[\"type\"],\n",
    "                \"valid_types\": list(valid_types),\n",
    "                \"item_id\": item.get(\"id\", \"unknown\")\n",
    "            })\n",
    "        \n",
    "        # Check for empty name\n",
    "        if \"name\" in item and (not item[\"name\"] or item[\"name\"].strip() == \"\"):\n",
    "            item_issues.append({\n",
    "                \"issue_type\": \"empty_name\",\n",
    "                \"item_id\": item.get(\"id\", \"unknown\")\n",
    "            })\n",
    "        \n",
    "        # Add additional validation rules here as needed\n",
    "        \n",
    "        # If no issues were found, consider the item valid\n",
    "        if not item_issues:\n",
    "            valid_items.append(item)\n",
    "        else:\n",
    "            # Add the item with quality issues to our list\n",
    "            for issue in item_issues:\n",
    "                issue_record = {\n",
    "                    \"item\": item,\n",
    "                    \"issue\": issue,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "                quality_issues.append(issue_record)\n",
    "                logging.warning(f\"Data quality issue: {issue['issue_type']} for item {issue['item_id']}\")\n",
    "    \n",
    "    logging.info(f\"Data quality validation complete: {len(valid_items)} valid items, \"\n",
    "                f\"{len(quality_issues)} quality issues\")\n",
    "    \n",
    "    return valid_items, quality_issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 5: Data Storage and Delta Lake Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_json(items: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Save the raw JSON data to a file for historical reference\n",
    "    \n",
    "    Args:\n",
    "        items (List[Dict]): The raw items to save\n",
    "        \n",
    "    Returns:\n",
    "        str: The path to the saved file\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    notebookutils.fs.mkdirs(config[\"raw_json_dir\"])\n",
    "    \n",
    "    # Generate filename with current date and time for historical tracking\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    fileName = f'Fabric_Items_{timestamp}.json'\n",
    "    file_path = f\"{config['raw_json_dir']}{fileName}\"\n",
    "    \n",
    "    try:\n",
    "        # Write the output to a JSON file\n",
    "        with open(file_path, \"w\") as json_file:\n",
    "            json.dump(items, json_file, indent=2)\n",
    "        logging.info(f\"Successfully saved raw data to {file_path}\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving JSON file: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_and_store_data(items: List[Dict], quality_issues: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the validated items and store them in Delta format\n",
    "    \n",
    "    Args:\n",
    "        items (List[Dict]): The validated items to process and store\n",
    "        quality_issues (List[Dict]): Quality issues to store separately\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataframe of items\n",
    "    \"\"\"\n",
    "    logging.info(\"Processing and normalizing data\")\n",
    "    \n",
    "    # Convert items to DataFrame\n",
    "    df = pd.json_normalize(items)\n",
    "    \n",
    "    # Check if we have any data to process\n",
    "    if df.empty:\n",
    "        logging.warning(\"No valid items to process after data quality validation\")\n",
    "        return df\n",
    "    \n",
    "    # Add processing timestamp\n",
    "    df['processing_timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    # Set storage options for writing to Delta format\n",
    "    storage_options = {\n",
    "        \"use_fabric_endpoint\": \"true\", \n",
    "        \"allow_unsafe_rename\": \"true\", \n",
    "        \"bearer_token\": notebookutils.credentials.getToken('storage')\n",
    "    }\n",
    "    \n",
    "    # Process the data in batches to avoid memory issues with large datasets\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    if total_rows > batch_size:\n",
    "        logging.info(f\"Processing {total_rows} rows in batches of {batch_size}\")\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, total_rows, batch_size):\n",
    "            end_idx = min(i + batch_size, total_rows)\n",
    "            batch_df = df.iloc[i:end_idx]\n",
    "            \n",
    "            # For the first batch, overwrite the staging table\n",
    "            mode = \"overwrite\" if i == 0 else \"append\"\n",
    "            \n",
    "            logging.info(f\"Writing batch {i//batch_size + 1} ({len(batch_df)} rows) to staging table with mode: {mode}\")\n",
    "            \n",
    "            write_deltalake(\n",
    "                config[\"staging_table_path\"], \n",
    "                batch_df, \n",
    "                mode=mode, \n",
    "                engine='rust', \n",
    "                storage_options=storage_options\n",
    "            )\n",
    "    else:\n",
    "        # If the dataset is small enough, write it all at once\n",
    "        logging.info(f\"Writing {total_rows} rows to staging table\")\n",
    "        \n",
    "        write_deltalake(\n",
    "            config[\"staging_table_path\"], \n",
    "            df, \n",
    "            mode=\"overwrite\", \n",
    "            engine='rust', \n",
    "            storage_options=storage_options\n",
    "        )\n",
    "    \n",
    "    # Store quality issues if any were found\n",
    "    if quality_issues:\n",
    "        logging.info(f\"Storing {len(quality_issues)} data quality issues\")\n",
    "        quality_df = pd.DataFrame(quality_issues)\n",
    "        \n",
    "        # Check if the quality issues table exists\n",
    "        quality_table_exists = notebookutils.fs.exists(config[\"quality_issues_path\"])\n",
    "        \n",
    "        write_deltalake(\n",
    "            config[\"quality_issues_path\"], \n",
    "            quality_df, \n",
    "            mode=\"append\" if quality_table_exists else \"overwrite\", \n",
    "            engine='rust', \n",
    "            storage_options=storage_options\n",
    "        )\n",
    "    \n",
    "    logging.info(\"Successfully processed and stored data in staging table\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 6: Change Tracking and History Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_record_changes(staging_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Detect changes between staging and production data, and record the history\n",
    "    \n",
    "    Args:\n",
    "        staging_df (pd.DataFrame): The dataframe with new/updated items\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: The final production dataframe and changes dataframe\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting change detection process\")\n",
    "    \n",
    "    # Check if production table exists\n",
    "    production_exists = notebookutils.fs.exists(config[\"production_table_path\"])\n",
    "    \n",
    "    if not production_exists:\n",
    "        logging.info(\"No existing production data - all items are new\")\n",
    "        \n",
    "        # Add change_type column to indicate these are all new items\n",
    "        staging_df['change_type'] = 'new'\n",
    "        \n",
    "        # The changes are all new items in this case\n",
    "        changes_df = staging_df.copy()\n",
    "        changes_df['change_timestamp'] = datetime.now().isoformat()\n",
    "        \n",
    "        # Return both dataframes\n",
    "        return staging_df, changes_df\n",
    "    \n",
    "    # Connect to DuckDB for performing SQL operations\n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    try:\n",
    "        # Install and load the Delta extension\n",
    "        con.execute(\"INSTALL delta;\")\n",
    "        con.execute(\"LOAD delta;\")\n",
    "        \n",
    "        # Load both staging and production data\n",
    "        con.execute(f\"CREATE TABLE staging AS SELECT * FROM delta_scan('{config['staging_table_path']}')\")\n",
    "        con.execute(f\"CREATE TABLE production AS SELECT * FROM delta_scan('{config['production_table_path']}')\")\n",
    "        \n",
    "        # Find new items (in staging but not in production)\n",
    "        new_items_query = \"\"\"\n",
    "            SELECT s.*, 'new' as change_type\n",
    "            FROM staging s\n",
    "            LEFT JOIN production p ON s.id = p.id\n",
    "            WHERE p.id IS NULL\n",
    "        \"\"\"\n",
    "        new_items = con.execute(new_items_query).fetchdf()\n",
    "        \n",
    "        # Find updated items (in both, but with changes)\n",
    "        # This assumes 'modifiedDate' is a reliable indicator of changes\n",
    "        updated_items_query = \"\"\"\n",
    "            SELECT s.*, 'updated' as change_type\n",
    "            FROM staging s\n",
    "            JOIN production p ON s.id = p.id\n",
    "            WHERE s.modifiedDate > p.modifiedDate\n",
    "        \"\"\"\n",
    "        updated_items = con.execute(updated_items_query).fetchdf()\n",
    "        \n",
    "        # Find deleted items (in production but not in staging)\n",
    "        # Only if we're doing a full refresh (if incremental, we can't detect deletions)\n",
    "        deleted_items = pd.DataFrame()\n",
    "        if not config[\"incremental_refresh\"]:\n",
    "            deleted_items_query = \"\"\"\n",
    "                SELECT p.*, 'deleted' as change_type\n",
    "                FROM production p\n",
    "                LEFT JOIN staging s ON p.id = s.id\n",
    "                WHERE s.id IS NULL\n",
    "            \"\"\"\n",
    "            deleted_items = con.execute(deleted_items_query).fetchdf()\n",
    "        \n",
    "        # Combine all changes\n",
    "        changes_df = pd.concat([new_items, updated_items, deleted_items], ignore_index=True)\n",
    "        \n",
    "        # Add timestamp for when the change was detected\n",
    "        changes_df['change_timestamp'] = datetime.now().isoformat()\n",
    "        \n",
    "        # If we're doing incremental refresh, we need to merge staging with production\n",
    "        if config[\"incremental_refresh\"]:\n",
    "            logging.info(\"Performing incremental merge of staging data into production\")\n",
    "            \n",
    "            # Merge updated items first\n",
    "            con.execute(\"\"\"\n",
    "                UPDATE production p\n",
    "                SET\n",
    "                    name = s.name,\n",
    "                    type = s.type,\n",
    "                    modifiedDate = s.modifiedDate,\n",
    "                    modifiedBy = s.modifiedBy,\n",
    "                    processing_timestamp = s.processing_timestamp\n",
    "                FROM staging s\n",
    "                WHERE p.id = s.id\n",
    "            \"\"\")\n",
    "            \n",
    "            # Insert new items\n",
    "            con.execute(\"\"\"\n",
    "                INSERT INTO production\n",
    "                SELECT s.*\n",
    "                FROM staging s\n",
    "                LEFT JOIN production p ON s.id = p.id\n",
    "                WHERE p.id IS NULL\n",
    "            \"\"\")\n",
    "            \n",
    "            # Get the final merged dataset\n",
    "            final_df = con.execute(\"SELECT * FROM production\").fetchdf()\n",
    "        else:\n",
    "            # For full refresh, we just use the staging data plus add back any deleted items marked as deleted\n",
    "            if not deleted_items.empty:\n",
    "                # Mark deleted items in the final dataset\n",
    "                deleted_items['is_deleted'] = True\n",
    "                \n",
    "                # Get only the necessary columns from staging\n",
    "                staging_columns = staging_df.columns.tolist()\n",
    "                \n",
    "                # Ensure deleted_items has the same columns as staging\n",
    "                for col in staging_columns:\n",
    "                    if col not in deleted_items.columns:\n",
    "                        deleted_items[col] = None\n",
    "                \n",
    "                # Combine staging with deleted items\n",
    "                final_df = pd.concat([staging_df, deleted_items[staging_columns]], ignore_index=True)\n",
    "            else:\n",
    "                final_df = staging_df\n",
    "        \n",
    "        # Record changes if any were detected\n",
    "        if not changes_df.empty:\n",
    "            logging.info(f\"Detected {len(changes_df)} changes: \"\n",
    "                        f\"{len(new_items)} new, {len(updated_items)} updated, {len(deleted_items)} deleted\")\n",
    "            \n",
    "            # Set storage options\n",
    "            storage_options = {\n",
    "                \"use_fabric_endpoint\": \"true\", \n",
    "                \"allow_unsafe_rename\": \"true\", \n",
    "                \"bearer_token\": notebookutils.credentials.getToken('storage')\n",
    "            }\n",
    "            \n",
    "            # Check if changes table exists\n",
    "            changes_table_exists = notebookutils.fs.exists(config[\"changes_table_path\"])\n",
    "            \n",
    "            # Write changes to changes tracking table\n",
    "            write_deltalake(\n",
    "                config[\"changes_table_path\"], \n",
    "                changes_df, \n",
    "                mode=\"append\" if changes_table_exists else \"overwrite\", \n",
    "                engine='rust', \n",
    "                storage_options=storage_options\n",
    "            )\n",
    "            \n",
    "            # Also append to history table\n",
    "            history_table_exists = notebookutils.fs.exists(config[\"history_table_path\"])\n",
    "            \n",
    "            write_deltalake(\n",
    "                config[\"history_table_path\"], \n",
    "                changes_df, \n",
    "                mode=\"append\" if history_table_exists else \"overwrite\", \n",
    "                engine='rust', \n",
    "                storage_options=storage_options\n",
    "            )\n",
    "        else:\n",
    "            logging.info(\"No changes detected between staging and production data\")\n",
    "        \n",
    "        return final_df, changes_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during change detection: {str(e)}\")\n",
    "        # If there's an error, just return the staging data\n",
    "        return staging_df, pd.DataFrame()\n",
    "\n",
    "def prune_history_data():\n",
    "    \"\"\"\n",
    "    Prune historical data to keep storage manageable by removing data older than the retention period\n",
    "    \"\"\"\n",
    "    logging.info(\"Checking if history data needs pruning\")\n",
    "    \n",
    "    # Check if history table exists\n",
    "    history_exists = notebookutils.fs.exists(config[\"history_table_path\"])\n",
    "    \n",
    "    if not history_exists:\n",
    "        logging.info(\"No history data exists yet, skipping pruning\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Calculate cutoff date\n",
    "        days_to_keep = config[\"days_to_keep_history\"]\n",
    "        cutoff_date = (datetime.now() - timedelta(days=days_to_keep)).isoformat()\n",
    "        \n",
    "        logging.info(f\"Pruning history data older than {cutoff_date}\")\n",
    "        \n",
    "        # Connect to DuckDB for SQL operations\n",
    "        con = duckdb.connect()\n",
    "        \n",
    "        # Install and load Delta extension\n",
    "        con.execute(\"INSTALL delta;\")\n",
    "        con.execute(\"LOAD delta;\")\n",
    "        \n",
    "        # Load history data\n",
    "        con.execute(f\"CREATE TABLE history AS SELECT * FROM delta_scan('{config['history_table_path']}')\")\n",
    "        \n",
    "        # Count total records before pruning\n",
    "        total_count = con.execute(\"SELECT COUNT(*) FROM history\").fetchone()[0]\n",
    "        \n",
    "        # Count records to be pruned\n",
    "        to_prune_count = con.execute(f\"\"\"\n",
    "            SELECT COUNT(*) FROM history \n",
    "            WHERE change_timestamp < '{cutoff_date}'\n",
    "        \"\"\").fetchone()[0]\n",
    "        \n",
    "        if to_prune_count == 0:\n",
    "            logging.info(\"No history data needs pruning at this time\")\n",
    "            return\n",
    "        \n",
    "        # Get records to keep\n",
    "        keep_records = con.execute(f\"\"\"\n",
    "            SELECT * FROM history \n",
    "            WHERE change_timestamp >= '{cutoff_date}'\n",
    "        \"\"\").fetchdf()\n",
    "        \n",
    "        if keep_records.empty:\n",
    "            logging.info(\"All history would be pruned, keeping at least 30 days worth instead\")\n",
    "            # Adjust to keep at least 30 days\n",
    "            adjusted_cutoff = (datetime.now() - timedelta(days=30)).isoformat()\n",
    "            keep_records = con.execute(f\"\"\"\n",
    "                SELECT * FROM history \n",
    "                WHERE change_timestamp >= '{adjusted_cutoff}'\n",
    "            \"\"\").fetchdf()\n",
    "        \n",
    "        # Write back only the records to keep\n",
    "        storage_options = {\n",
    "            \"use_fabric_endpoint\": \"true\", \n",
    "            \"allow_unsafe_rename\": \"true\", \n",
    "            \"bearer_token\": notebookutils.credentials.getToken('storage')\n",
    "        }\n",
    "        \n",
    "        write_deltalake(\n",
    "            config[\"history_table_path\"], \n",
    "            keep_records, \n",
    "            mode=\"overwrite\", \n",
    "            engine='rust', \n",
    "            storage_options=storage_options\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Pruned {to_prune_count} out of {total_count} historical records older than {cutoff_date}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during history pruning: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 7: Final Data Update and Alert System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email_alert(subject: str, message: str):\n",
    "    \"\"\"\n",
    "    Send an email alert when significant changes are detected\n",
    "    \n",
    "    Args:\n",
    "        subject (str): Email subject\n",
    "        message (str): Email message body\n",
    "    \"\"\"\n",
    "    if not config[\"enable_alerts\"]:\n",
    "        logging.info(\"Alerts are disabled, skipping email notification\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Create email message\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = config[\"alert_sender_email\"]\n",
    "        msg['To'] = \", \".join(config[\"alert_email_recipients\"])\n",
    "        msg['Subject'] = subject\n",
    "        \n",
    "        # Add message body\n",
    "        msg.attach(MIMEText(message, 'plain'))\n",
    "        \n",
    "        # Connect to SMTP server\n",
    "        server = smtplib.SMTP(config[\"alert_smtp_server\"], config[\"alert_smtp_port\"])\n",
    "        server.starttls()\n",
    "        \n",
    "        # Send email\n",
    "        server.send_message(msg)\n",
    "        server.quit()\n",
    "        \n",
    "        logging.info(f\"Email alert sent: {subject}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to send email alert: {str(e)}\")\n",
    "\n",
    "def check_for_alert_conditions(changes_df: pd.DataFrame, staging_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Check if any alert conditions are met based on the detected changes\n",
    "    \n",
    "    Args:\n",
    "        changes_df (pd.DataFrame): DataFrame containing detected changes\n",
    "        staging_df (pd.DataFrame): DataFrame containing all current items\n",
    "    \"\"\"\n",
    "    # Skip if changes DataFrame is empty or alerts are disabled\n",
    "    if changes_df.empty or not config[\"enable_alerts\"]:\n",
    "        return\n",
    "    \n",
    "    alerts = []\n",
    "    \n",
    "    # Check for significant change in item count\n",
    "    try:\n",
    "        production_exists = notebookutils.fs.exists(config[\"production_table_path\"])\n",
    "        \n",
    "        if production_exists:\n",
    "            # Read the production table to get the previous count\n",
    "            production_df = read_deltalake(config[\"production_table_path\"]).to_pandas()\n",
    "            previous_count = len(production_df)\n",
    "            current_count = len(staging_df)\n",
    "            \n",
    "            # Calculate percentage change\n",
    "            if previous_count > 0:\n",
    "                pct_change = abs((current_count - previous_count) / previous_count * 100)\n",
    "                \n",
    "                # Alert if the change exceeds the configured threshold\n",
    "                if pct_change > config[\"alert_threshold_item_count_change_pct\"]:\n",
    "                    alert_msg = (f\"ALERT: Significant change in Fabric item count detected. \"\n",
    "                                f\"Previous: {previous_count}, Current: {current_count}, \"\n",
    "                                f\"Change: {pct_change:.2f}%\")\n",
    "                    alerts.append(alert_msg)\n",
    "                    logging.warning(alert_msg)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error checking for item count changes: {str(e)}\")\n",
    "    \n",
    "    # Check for specific item type changes\n",
    "    try:\n",
    "        # Get counts of changes by change_type and item_type\n",
    "        change_summary = changes_df.groupby(['change_type', 'type']).size().reset_index(name='count')\n",
    "        \n",
    "        # Check for significant changes in specific item types\n",
    "        for _, row in change_summary.iterrows():\n",
    "            change_type = row['change_type']\n",
    "            item_type = row['type']\n",
    "            count = row['count']\n",
    "            \n",
    "            # Set thresholds based on item type (these could be moved to config)\n",
    "            threshold = 5  # Default threshold\n",
    "            if item_type in ['Report', 'Dataset']:\n",
    "                threshold = 10\n",
    "            elif item_type in ['Lakehouse', 'Warehouse']:\n",
    "                threshold = 3  # More critical items have lower thresholds\n",
    "            \n",
    "            if count > threshold:\n",
    "                alert_msg = (f\"ALERT: {count} {item_type} items have been {change_type}. \"\n",
    "                            f\"This exceeds the threshold of {threshold}.\")\n",
    "                alerts.append(alert_msg)\n",
    "                logging.warning(alert_msg)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error checking for item type changes: {str(e)}\")\n",
    "    \n",
    "    # Check for changes in critically important items (e.g., by specific workspaces)\n",
    "    try:\n",
    "        # Define critical workspaces (could be moved to config)\n",
    "        critical_workspaces = ['Finance', 'Executive', 'Production']\n",
    "        \n",
    "        # Filter changes to only those in critical workspaces\n",
    "        if 'workspaceName' in changes_df.columns:\n",
    "            critical_changes = changes_df[changes_df['workspaceName'].str.contains('|'.join(critical_workspaces), \n",
    "                                                                                  case=False, \n",
    "                                                                                  na=False)]\n",
    "            \n",
    "            if not critical_changes.empty:\n",
    "                alert_msg = (f\"ALERT: {len(critical_changes)} changes detected in critical workspaces: \"\n",
    "                            f\"{', '.join(critical_workspaces)}.\")\n",
    "                alerts.append(alert_msg)\n",
    "                logging.warning(alert_msg)\n",
    "                \n",
    "                # Add details for each critical change\n",
    "                for _, row in critical_changes.iterrows():\n",
    "                    item_detail = (f\"- {row.get('change_type', 'Changed')} item: {row.get('name', 'Unknown')} \"\n",
    "                                  f\"in workspace: {row.get('workspaceName', 'Unknown')}\")\n",
    "                    alerts.append(item_detail)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error checking for critical workspace changes: {str(e)}\")\n",
    "    \n",
    "    # If we have alerts, send an email\n",
    "    if alerts:\n",
    "        subject = f\"Fabric Items Alert: {len(alerts)} significant changes detected\"\n",
    "        message = \"\\n\".join(alerts)\n",
    "        message += f\"\\n\\nFor more details, check the log file: {log_file_path}\"\n",
    "        \n",
    "        send_email_alert(subject, message)\n",
    "\n",
    "def update_production_data_and_history(staging_df: pd.DataFrame, changes_df: pd.DataFrame = None):\n",
    "    \"\"\"\n",
    "    Update the production table with new data and maintain historical records\n",
    "    \n",
    "    Args:\n",
    "        staging_df (pd.DataFrame): DataFrame containing all current Fabric items\n",
    "        changes_df (pd.DataFrame, optional): DataFrame containing detected changes\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert DataFrames to Arrow tables for Delta Lake operations\n",
    "        staging_table = pyarrow.Table.from_pandas(staging_df)\n",
    "        \n",
    "        # Update the production table (overwrite with the latest data)\n",
    "        logging.info(f\"Updating production table at {config['production_table_path']}\")\n",
    "        write_deltalake(\n",
    "            config[\"production_table_path\"],\n",
    "            staging_table,\n",
    "            mode=\"overwrite\",\n",
    "            engine=\"pyarrow\"\n",
    "        )\n",
    "        \n",
    "        # Update the history table (append with current snapshot and timestamp)\n",
    "        history_df = staging_df.copy()\n",
    "        history_df[\"snapshot_date\"] = datetime.now().isoformat()\n",
    "        \n",
    "        history_table = pyarrow.Table.from_pandas(history_df)\n",
    "        \n",
    "        logging.info(f\"Updating history table at {config['history_table_path']}\")\n",
    "        write_deltalake(\n",
    "            config[\"history_table_path\"],\n",
    "            history_table,\n",
    "            mode=\"append\",\n",
    "            engine=\"pyarrow\"\n",
    "        )\n",
    "        \n",
    "        # If changes were detected, save them to the changes table\n",
    "        if changes_df is not None and not changes_df.empty:\n",
    "            changes_table = pyarrow.Table.from_pandas(changes_df)\n",
    "            \n",
    "            logging.info(f\"Updating changes table at {config['changes_table_path']} with {len(changes_df)} changes\")\n",
    "            write_deltalake(\n",
    "                config[\"changes_table_path\"],\n",
    "                changes_table,\n",
    "                mode=\"append\",\n",
    "                engine=\"pyarrow\"\n",
    "            )\n",
    "        \n",
    "        # Clean up historical data to maintain retention policy\n",
    "        if config[\"days_to_keep_history\"] > 0:\n",
    "            cleanup_historical_data(config[\"days_to_keep_history\"])\n",
    "        \n",
    "        logging.info(\"Data update complete\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error updating production data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def cleanup_historical_data(days_to_keep: int):\n",
    "    \"\"\"\n",
    "    Remove historical data older than the specified retention period\n",
    "    \n",
    "    Args:\n",
    "        days_to_keep (int): Number of days of history to retain\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate the cutoff date\n",
    "        cutoff_date = (datetime.now() - timedelta(days=days_to_keep)).isoformat()\n",
    "        \n",
    "        # Check if history table exists\n",
    "        if not notebookutils.fs.exists(config[\"history_table_path\"]):\n",
    "            logging.info(\"No history table found, skipping cleanup\")\n",
    "            return\n",
    "            \n",
    "        # Read the history table\n",
    "        history_df = read_deltalake(config[\"history_table_path\"]).to_pandas()\n",
    "        \n",
    "        # Filter to keep only records within the retention period\n",
    "        retained_history = history_df[history_df[\"snapshot_date\"] >= cutoff_date]\n",
    "        \n",
    "        # Check if we have records to remove\n",
    "        if len(retained_history) < len(history_df):\n",
    "            records_removed = len(history_df) - len(retained_history)\n",
    "            logging.info(f\"Removing {records_removed} historical records older than {cutoff_date}\")\n",
    "            \n",
    "            # Write back the filtered history\n",
    "            retained_table = pyarrow.Table.from_pandas(retained_history)\n",
    "            write_deltalake(\n",
    "                config[\"history_table_path\"],\n",
    "                retained_table,\n",
    "                mode=\"overwrite\",\n",
    "                engine=\"pyarrow\"\n",
    "            )\n",
    "        else:\n",
    "            logging.info(f\"No historical records found older than {cutoff_date}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during historical data cleanup: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 8: Create Views for Common Analysis Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_views():\n",
    "    \"\"\"\n",
    "    Create SQL views for common analysis patterns using DuckDB\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Creating analysis views for common patterns\")\n",
    "        \n",
    "        # Initialize DuckDB connection\n",
    "        conn = duckdb.connect(database=':memory:')\n",
    "        \n",
    "        # Register the Delta table as a view in DuckDB\n",
    "        conn.execute(f\"INSTALL 'delta';\")\n",
    "        conn.execute(f\"LOAD 'delta';\")\n",
    "        \n",
    "        # Register the tables\n",
    "        conn.execute(f\"CREATE VIEW fabric_items AS SELECT * FROM delta_scan('{config['production_table_path']}');\")\n",
    "        conn.execute(f\"CREATE VIEW fabric_items_history AS SELECT * FROM delta_scan('{config['history_table_path']}');\")\n",
    "        conn.execute(f\"CREATE VIEW fabric_items_changes AS SELECT * FROM delta_scan('{config['changes_table_path']}');\")\n",
    "        \n",
    "        # Create view 1: Usage trends by item type\n",
    "        view_sql = \"\"\"\n",
    "        CREATE VIEW IF NOT EXISTS usage_trends_by_type AS\n",
    "        SELECT \n",
    "            DATE_TRUNC('month', snapshot_date::TIMESTAMP) AS month,\n",
    "            type,\n",
    "            COUNT(*) AS item_count,\n",
    "            AVG(views) AS avg_views,\n",
    "            SUM(views) AS total_views\n",
    "        FROM fabric_items_history\n",
    "        GROUP BY 1, 2\n",
    "        ORDER BY 1, 2;\n",
    "        \"\"\"\n",
    "        conn.execute(view_sql)\n",
    "        \n",
    "        # Create view 2: Recently modified items\n",
    "        view_sql = \"\"\"\n",
    "        CREATE VIEW IF NOT EXISTS recently_modified_items AS\n",
    "        SELECT \n",
    "            id,\n",
    "            name,\n",
    "            type,\n",
    "            workspaceName,\n",
    "            modifiedBy,\n",
    "            modifiedDate,\n",
    "            DATEDIFF('day', modifiedDate::TIMESTAMP, CURRENT_TIMESTAMP) AS days_since_modified\n",
    "        FROM fabric_items\n",
    "        WHERE DATEDIFF('day', modifiedDate::TIMESTAMP, CURRENT_TIMESTAMP) <= 30\n",
    "        ORDER BY modifiedDate DESC;\n",
    "        \"\"\"\n",
    "        conn.execute(view_sql)\n",
    "        \n",
    "        # Create view 3: Workspace growth over time\n",
    "        view_sql = \"\"\"\n",
    "        CREATE VIEW IF NOT EXISTS workspace_growth AS\n",
    "        SELECT \n",
    "            DATE_TRUNC('month', snapshot_date::TIMESTAMP) AS month,\n",
    "            workspaceName,\n",
    "            COUNT(DISTINCT id) AS item_count\n",
    "        FROM fabric_items_history\n",
    "        GROUP BY 1, 2\n",
    "        ORDER BY 1, 2;\n",
    "        \"\"\"\n",
    "        conn.execute(view_sql)\n",
    "        \n",
    "        # Create view 4: Most active users\n",
    "        view_sql = \"\"\"\n",
    "        CREATE VIEW IF NOT EXISTS most_active_users AS\n",
    "        SELECT \n",
    "            modifiedBy,\n",
    "            COUNT(*) AS items_modified,\n",
    "            COUNT(DISTINCT type) AS item_types_modified,\n",
    "            MIN(modifiedDate) AS first_modification,\n",
    "            MAX(modifiedDate) AS last_modification\n",
    "        FROM fabric_items\n",
    "        GROUP BY 1\n",
    "        ORDER BY 2 DESC;\n",
    "        \"\"\"\n",
    "        conn.execute(view_sql)\n",
    "        \n",
    "        # Create view 5: Item change frequency\n",
    "        view_sql = \"\"\"\n",
    "        CREATE VIEW IF NOT EXISTS item_change_frequency AS\n",
    "        SELECT \n",
    "            id,\n",
    "            name,\n",
    "            type,\n",
    "            workspaceName,\n",
    "            COUNT(*) AS change_count\n",
    "        FROM fabric_items_changes\n",
    "        GROUP BY 1, 2, 3, 4\n",
    "        ORDER BY 5 DESC;\n",
    "        \"\"\"\n",
    "        conn.execute(view_sql)\n",
    "        \n",
    "        # Save the views as Delta tables for querying from other tools\n",
    "        views = [\n",
    "            \"usage_trends_by_type\", \n",
    "            \"recently_modified_items\", \n",
    "            \"workspace_growth\", \n",
    "            \"most_active_users\", \n",
    "            \"item_change_frequency\"\n",
    "        ]\n",
    "        \n",
    "        for view in views:\n",
    "            result_df = conn.execute(f\"SELECT * FROM {view}\").fetchdf()\n",
    "            result_table = pyarrow.Table.from_pandas(result_df)\n",
    "            \n",
    "            # Write to the views folder as a Delta table\n",
    "            view_path = f\"/lakehouse/default/Tables/views/{view}\"\n",
    "            \n",
    "            # Create directory if it doesn't exist\n",
    "            notebookutils.fs.mkdirs(f\"Tables/views/\")\n",
    "            \n",
    "            write_deltalake(\n",
    "                view_path,\n",
    "                result_table,\n",
    "                mode=\"overwrite\",\n",
    "                engine=\"pyarrow\"\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Created analysis view: {view}\")\n",
    "        \n",
    "        logging.info(\"Analysis views creation complete\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating analysis views: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 9: Incremental Refresh Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_incremental_data(new_items: List[Dict], existing_data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process data for incremental refresh, identifying changes and updates\n",
    "    \n",
    "    Args:\n",
    "        new_items (List[Dict]): List of newly retrieved items\n",
    "        existing_data_path (str): Path to existing data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: Complete updated dataset and changes dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Processing incremental data update\")\n",
    "        \n",
    "        # Create DataFrame from new items\n",
    "        new_df = pd.DataFrame(new_items)\n",
    "        \n",
    "        # If no new items, return empty DataFrames\n",
    "        if new_df.empty:\n",
    "            logging.info(\"No new items to process\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "        # Add retrieval timestamp\n",
    "        new_df[\"retrieval_timestamp\"] = datetime.now().isoformat()\n",
    "        \n",
    "        # Check if existing data exists\n",
    "        if not notebookutils.fs.exists(existing_data_path):\n",
    "            logging.info(\"No existing data found, treating as initial load\")\n",
    "            # For initial load, mark all as additions\n",
    "            new_df[\"change_type\"] = \"added\"\n",
    "            return new_df, new_df\n",
    "            \n",
    "        # Load existing data\n",
    "        existing_df = read_deltalake(existing_data_path).to_pandas()\n",
    "        \n",
    "        # Create sets of existing and new IDs for comparison\n",
    "        existing_ids = set(existing_df[\"id\"])\n",
    "        new_ids = set(new_df[\"id\"])\n",
    "        \n",
    "        # Identify added, removed, and potentially updated items\n",
    "        added_ids = new_ids - existing_ids\n",
    "        removed_ids = existing_ids - new_ids\n",
    "        potentially_updated_ids = new_ids.intersection(existing_ids)\n",
    "        \n",
    "        # Prepare change tracking DataFrame\n",
    "        changes_df = pd.DataFrame()\n",
    "        \n",
    "        # Process added items\n",
    "        if added_ids:\n",
    "            added_df = new_df[new_df[\"id\"].isin(added_ids)].copy()\n",
    "            added_df[\"change_type\"] = \"added\"\n",
    "            changes_df = pd.concat([changes_df, added_df])\n",
    "            logging.info(f\"Identified {len(added_ids)} new items\")\n",
    "        \n",
    "        # Process removed items\n",
    "        if removed_ids:\n",
    "            removed_df = existing_df[existing_df[\"id\"].isin(removed_ids)].copy()\n",
    "            removed_df[\"change_type\"] = \"removed\"\n",
    "            changes_df = pd.concat([changes_df, removed_df])\n",
    "            logging.info(f\"Identified {len(removed_ids)} removed items\")\n",
    "        \n",
    "        # Process updated items - need to compare fields to detect changes\n",
    "        if potentially_updated_ids:\n",
    "            # Fields to compare for detecting updates\n",
    "            compare_fields = [\"name\", \"type\", \"modifiedBy\", \"modifiedDate\", \"views\"]\n",
    "            compare_fields = [f for f in compare_fields if f in new_df.columns and f in existing_df.columns]\n",
    "            \n",
    "            updated_items = []\n",
    "            \n",
    "            # Create lookup dictionary for faster comparison\n",
    "            existing_lookup = {row[\"id\"]: row for _, row in existing_df[existing_df[\"id\"].isin(potentially_updated_ids)].iterrows()}\n",
    "            \n",
    "            # Check each potentially updated item\n",
    "            for _, row in new_df[new_df[\"id\"].isin(potentially_updated_ids)].iterrows():\n",
    "                item_id = row[\"id\"]\n",
    "                existing_item = existing_lookup.get(item_id)\n",
    "                \n",
    "                if existing_item is not None:\n",
    "                    # Check if any fields changed\n",
    "                    for field in compare_fields:\n",
    "                        if row.get(field) != existing_item.get(field):\n",
    "                            # This item has been updated\n",
    "                            updated_item = row.copy()\n",
    "                            updated_item[\"change_type\"] = \"updated\"\n",
    "                            updated_item[\"previous_\" + field] = existing_item.get(field)\n",
    "                            updated_items.append(updated_item)\n",
    "                            break\n",
    "            \n",
    "            if updated_items:\n",
    "                updated_df = pd.DataFrame(updated_items)\n",
    "                changes_df = pd.concat([changes_df, updated_df])\n",
    "                logging.info(f\"Identified {len(updated_items)} updated items\")\n",
    "        \n",
    "        # Create complete updated dataset\n",
    "        # Remove items that no longer exist\n",
    "        filtered_existing_df = existing_df[~existing_df[\"id\"].isin(removed_ids)]\n",
    "        \n",
    "        # Update existing items with new data\n",
    "        for item_id in potentially_updated_ids:\n",
    "            existing_idx = filtered_existing_df.index[filtered_existing_df[\"id\"] == item_id].tolist()\n",
    "            new_idx = new_df.index[new_df[\"id\"] == item_id].tolist()\n",
    "            \n",
    "            if existing_idx and new_idx:\n",
    "                filtered_existing_df.loc[existing_idx[0]] = new_df.loc[new_idx[0]]\n",
    "        \n",
    "        # Add completely new items\n",
    "        added_df = new_df[new_df[\"id\"].isin(added_ids)]\n",
    "        updated_df = pd.concat([filtered_existing_df, added_df])\n",
    "        \n",
    "        logging.info(f\"Incremental update processed: {len(changes_df)} changes identified\")\n",
    "        \n",
    "        return updated_df, changes_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during incremental data processing: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 10: Main Execution Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution flow for Fabric Items retrieval and processing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting Fabric Items retrieval and management process\")\n",
    "        \n",
    "        # Step 1: Get authentication token\n",
    "        access_token = get_auth_token()\n",
    "        \n",
    "        # Step 2: Retrieve Fabric items from API\n",
    "        fabric_items = retrieve_fabric_items(access_token)\n",
    "        \n",
    "        if not fabric_items:\n",
    "            logging.warning(\"No Fabric items retrieved, aborting execution\")\n",
    "            return\n",
    "            \n",
    "        logging.info(f\"Retrieved {len(fabric_items)} Fabric items from API\")\n",
    "        \n",
    "        # Step 3: Process data with incremental refresh support\n",
    "        if config[\"incremental_refresh\"]:\n",
    "            full_df, changes_df = process_incremental_data(fabric_items, config[\"production_table_path\"])\n",
    "            \n",
    "            # Skip further processing if no data or changes\n",
    "            if full_df.empty:\n",
    "                logging.info(\"No data to process after incremental refresh\")\n",
    "                return\n",
    "        else:\n",
    "            # Full refresh mode - process all items as new\n",
    "            full_df = pd.DataFrame(fabric_items)\n",
    "            full_df[\"retrieval_timestamp\"] = datetime.now().isoformat()\n",
    "            changes_df = pd.DataFrame()  # No change tracking in full refresh mode\n",
    "        \n",
    "        # Step 4: Perform data quality validation (would need implementation)\n",
    "        valid_items, quality_issues = validate_data_quality(fabric_items)\n",
    "        logging.info(f\"Data quality validation: {len(valid_items)} valid, {len(quality_issues)} issues\")\n",
    "    \n",
    "        full_df = process_and_store_data(valid_items, quality_issues)\n",
    "        \n",
    "        # Step 5: Check for alert conditions\n",
    "        check_for_alert_conditions(changes_df, full_df)\n",
    "        \n",
    "        # Step 6: Update production data and maintain history\n",
    "        update_production_data_and_history(full_df, changes_df)\n",
    "        \n",
    "        # Step 7: Create or update analysis views\n",
    "        create_analysis_views()\n",
    "        \n",
    "        logging.info(\"Fabric Items retrieval and management process completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {str(e)}\")\n",
    "        send_email_alert(\n",
    "            \"Fabric Items Process Error\",\n",
    "            f\"An error occurred during the Fabric Items retrieval and management process: {str(e)}\"\n",
    "        )\n",
    "\n",
    "# Execute the main function if running as the main script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 11: Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_validate_data_quality():\n",
    "    items = [{\"id\": \"1\", \"name\": \"\", \"type\": \"Report\", \"views\": \"-1\", \"modifiedDate\": \"invalid\"}]\n",
    "    valid_items, issues = validate_data_quality(items)\n",
    "    assert len(issues) >= 3  # Expect issues for empty name, invalid views, and invalid date\n",
    "    assert len(valid_items) == 0  # No valid items due to multiple issues\n",
    "    print(\"Test passed: validate_data_quality\")\n",
    "\n",
    "test_validate_data_quality()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
