{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Microsoft Fabric Items Retrieval via REST API\n",
    "Last Updated: March 2025\n",
    "Purpose: \n",
    "- Retrieve all Fabric items efficiently with minimal Capacity Unit usage\n",
    "- Store results in Delta format for analysis and tracking\n",
    " \n",
    "IMPORTANT NOTES:\n",
    "- As of March 2025, Dataflow Gen2 is not included in the Fabric items\n",
    "- This only retrieves Fabric Items, not Power BI Items\n",
    "- Using this Python notebook uses significantly less Capacity Units than a Spark notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import notebookutils\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime, date, timedelta\n",
    "import time\n",
    "import duckdb\n",
    "from deltalake import write_deltalake, DeltaTable\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 1: Authentication Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve secrets from Azure Key Vault for secure authentication\n",
    "# The key vault stores sensitive information like tenant ID, application ID and client secret\n",
    "key_vault = \"https://company-keyvault.vault.azure.net/\"\n",
    "\n",
    "# Tenant ID for the Azure AD tenant housing the Fabric workspace\n",
    "tenant = notebookutils.credentials.getSecret(key_vault, \"tenantid\") \n",
    "\n",
    "# Application ID for the service principal with appropriate Fabric permissions\n",
    "client = notebookutils.credentials.getSecret(key_vault, \"powerbi-applicationid\") \n",
    "\n",
    "# Client secret for authentication with the service principal\n",
    "client_secret = notebookutils.credentials.getSecret(key_vault, \"powerbi-clientsecret\")  \n",
    "\n",
    "# Import required authentication libraries with error handling\n",
    "try: \n",
    "    from azure.identity import ClientSecretCredential \n",
    "except Exception:\n",
    "    # Install the library if not available\n",
    "    !pip install azure.identity \n",
    "    from azure.identity import ClientSecretCredential \n",
    "\n",
    "# Set up authentication parameters\n",
    "# Using the Power BI API scope since Fabric API leverages the same authentication framework\n",
    "api = 'https://analysis.windows.net/powerbi/api/.default' \n",
    "\n",
    "# Create the credential object for service principal authentication\n",
    "auth = ClientSecretCredential(\n",
    "    authority = 'https://login.microsoftonline.com/', \n",
    "    tenant_id = tenant, \n",
    "    client_id = client, \n",
    "    client_secret = client_secret\n",
    ") \n",
    "\n",
    "# Retrieve the access token that will be used for all API calls\n",
    "access_token = auth.get_token(api)\n",
    "access_token = access_token.token \n",
    "\n",
    "# Set up HTTP headers with the access token for API authentication\n",
    "header = {'Authorization': f'Bearer {access_token}'}  \n",
    "\n",
    "print('\\nSuccessfully authenticated to Microsoft Fabric API.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2: API Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Microsoft Fabric API endpoint for retrieving items\n",
    "# This endpoint provides all Fabric items that the service principal has access to\n",
    "FABRIC_API_URL = 'https://api.fabric.microsoft.com/v1/admin/items'\n",
    "\n",
    "# Set up headers with authentication token and content type\n",
    "headers = {\n",
    "    'Authorization': 'Bearer ' + access_token,\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store all Fabric items\n",
    "# We'll collect all pages of results through the pagination mechanism\n",
    "all_fabric_items = []\n",
    "\n",
    "# Initialize the first API request\n",
    "# We'll update this URL with each pagination link for subsequent requests\n",
    "current_api_url = FABRIC_API_URL\n",
    "\n",
    "# Implement pagination to retrieve all items\n",
    "# The Fabric API returns data in pages, and we need to follow the @odata.nextLink to get all items\n",
    "print(\"Starting Fabric items retrieval...\")\n",
    "page_count = 0\n",
    "\n",
    "# Begin pagination loop\n",
    "while current_api_url:\n",
    "    # Make the API request for the current page\n",
    "    response = requests.get(current_api_url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error retrieving data: Status code {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        break\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    page_count += 1\n",
    "    \n",
    "    # Extract and store the Fabric items from the current page\n",
    "    items_in_page = data.get('itemEntities', [])\n",
    "    all_fabric_items.extend(items_in_page)\n",
    "    print(f\"Retrieved page {page_count} with {len(items_in_page)} items\")\n",
    "    \n",
    "    # Get the URL for the next page of results, if available\n",
    "    # If there are no more pages, this will be None and the loop will exit\n",
    "    current_api_url = data.get('@odata.nextLink')\n",
    "    \n",
    "    # Optional: Add a small delay to avoid rate limiting\n",
    "    if current_api_url:\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print(f\"Completed retrieval of {len(all_fabric_items)} Fabric items across {page_count} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 3: Save Raw JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create directory for storing the raw JSON data if it doesn't already exist\n",
    "# This allows us to keep historical snapshots of the Fabric items\n",
    "notebookutils.fs.mkdirs(\"Files/Fabric_Items/\")\n",
    "\n",
    "# Generate filename with current date for historical tracking\n",
    "fileName = 'Fabric_Items_' + (datetime.today()).strftime('%Y%m%d') + '.json'\n",
    "file_path = f\"/lakehouse/default/Files/Fabric_Items/{fileName}\"\n",
    "\n",
    "# Write the output to a JSON file\n",
    "try:\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(all_fabric_items, json_file, indent=4)\n",
    "    print(f\"Successfully saved raw data to {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 4: Process and Store in Delta Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the JSON file we just saved\n",
    "# This step can be adjusted if you want to process the data directly without saving to JSON first\n",
    "try:\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Flatten the nested JSON structure using pandas json_normalize\n",
    "    # This converts the hierarchical JSON into a flat dataframe for easier analysis\n",
    "    df = pd.json_normalize(data)\n",
    "    print(f\"Successfully loaded and normalized data with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    \n",
    "    # Set storage options for writing to Delta format\n",
    "    # The bearer token is required for authentication to the storage\n",
    "    storage_options = {\n",
    "        \"use_fabric_endpoint\": \"true\", \n",
    "        \"allow_unsafe_rename\": \"true\", \n",
    "        \"bearer_token\": notebookutils.credentials.getToken('storage')\n",
    "    }\n",
    "    \n",
    "    # Path for the staging table where we'll initially store the data\n",
    "    staging_path = f\"/lakehouse/default/Tables/staging_all_fabric_items\"\n",
    "    \n",
    "    # Write dataframe to Delta format in the staging location\n",
    "    # Using 'overwrite' mode to replace any existing data\n",
    "    write_deltalake(\n",
    "        staging_path, \n",
    "        df, \n",
    "        mode=\"overwrite\", \n",
    "        engine='rust', \n",
    "        storage_options=storage_options\n",
    "    )\n",
    "    print(f\"Successfully wrote data to staging table at {staging_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in data processing or Delta write: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 5: Merge Data with Existing Records (Upsert Operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to DuckDB for performing the merge operation\n",
    "# DuckDB provides efficient SQL operations on local data\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Install and load the Delta extension to enable DuckDB to work with Delta tables\n",
    "try:\n",
    "    con.execute(\"INSTALL delta;\")\n",
    "    con.execute(\"LOAD delta;\")\n",
    "    print(\"DuckDB Delta extension loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Delta extension: {e}\")\n",
    "\n",
    "# Define paths for source (staging) and target (production) tables\n",
    "source_path = \"/lakehouse/default/Tables/staging_all_fabric_items\"\n",
    "target_path = \"/lakehouse/default/Tables/all_fabric_items\"\n",
    "\n",
    "try:\n",
    "    # Register Delta tables in DuckDB\n",
    "    # Create temporary tables that point to our Delta tables\n",
    "    # Note: If target doesn't exist yet (first run), this will create it\n",
    "    con.execute(f\"CREATE TABLE source_table AS SELECT * FROM delta_scan('{source_path}')\")\n",
    "    \n",
    "    # Check if target table exists\n",
    "    try:\n",
    "        con.execute(f\"CREATE TABLE target_table AS SELECT * FROM delta_scan('{target_path}')\")\n",
    "        target_exists = True\n",
    "    except Exception:\n",
    "        print(\"Target table doesn't exist yet - this appears to be the first run\")\n",
    "        target_exists = False\n",
    "    \n",
    "    # If target exists, perform UPDATE and INSERT operations (upsert)\n",
    "    if target_exists:\n",
    "        # Update existing records\n",
    "        con.execute(\"\"\"\n",
    "            UPDATE target_table AS t\n",
    "            SET name = s.name,\n",
    "                type = s.type\n",
    "            FROM source_table AS s\n",
    "            WHERE t.id = s.id\n",
    "        \"\"\")\n",
    "        \n",
    "        # Insert new records\n",
    "        con.execute(\"\"\"\n",
    "            INSERT INTO target_table\n",
    "            SELECT s.*\n",
    "            FROM source_table AS s\n",
    "            LEFT JOIN target_table AS t ON s.id = t.id\n",
    "            WHERE t.id IS NULL\n",
    "        \"\"\")\n",
    "        \n",
    "        # Get the final dataset\n",
    "        result = con.execute(\"SELECT * FROM target_table\").fetchdf()\n",
    "    else:\n",
    "        # For first run, just use the source data\n",
    "        result = con.execute(\"SELECT * FROM source_table\").fetchdf()\n",
    "    \n",
    "    print(f\"Final dataset has {len(result)} rows\")\n",
    "    display(result)\n",
    "    \n",
    "    # Write the final dataset back to Delta storage\n",
    "    # Using a temporary \"New\" table that we can swap in production later if needed\n",
    "    write_deltalake(\n",
    "        f\"/lakehouse/default/Tables/all_fabric_items\", \n",
    "        result,\n",
    "        engine='rust',\n",
    "        mode=\"overwrite\",  \n",
    "        storage_options={\"allow_unsafe_rename\":\"true\"}\n",
    "    )\n",
    "    print(\"Successfully updated the production Fabric items table\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during merge operation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
